{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# Who's the Most Consistent Player in the NBA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sports fans and commentators prize \"consistency.\" There's an obvious intuitive appeal to the concept: we want to know that players are not just usually good but can be relied upon to be good every night. That said, \"consistency\" is usually defined in a vague or subjective manner within a sports context. For example, one of the recurring themes of the famously snarky 2000s sports blog \"Fire Joe Morgan\" was the (ironically) inconsistent way that sports commentators tended to use the term. \n",
    "\n",
    "So how can we define \"consistency\" in a consistent, robust way? Several popular pieces have tried to formalize this concept (e.g., https://www.theringer.com/nba/2020/1/6/21050900/most-least-consistent-nba-player), but I have found these efforts unsatisfactory. In this notebook, I'll try to do build a better consistency metric via the statistical concepts of <i>standard deviation</i> and <i>entropy</i>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import scipy\n",
    "\n",
    "import calculate\n",
    "import scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(scrape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of this analysis, we're going to acquire all player-level game logs from basketball-reference.com. This is a comprehensive website that is relatively easy to scrape. The site keeps an index of all player pages, organized alphabetically, on URLs that are formatted 'https://www.basketball-reference.com/players/{letter}/'. We can acquire all player homepage URLs from those pages, as well as the years in which the players were active. Once we have that information, we can rely on the fact that all player game-logs URLs are formatted 'https://www.basketball-reference.com/players/{letter}/{player}/gamelog/{year}' to systematically query every year's games for every player in the league's history. Once we've done that, it is relatively simple to use the BeautifulSoup library to locate the table of statistics, find all column names, and store each game's values in a dictionary.\n",
    "\n",
    "The basketball-reference.com website is, fortunately, friendly to these kinds of exercises and does not work very hard to rate-limit requests. So long as we include a small amount of error-handling logic in our scraping script to handle occasional timeouts and a simple retry step, we can be confident that we are getting a comprehsive account of all player gamelogs. To accelerate the process, I have multi-threaded the requests, which reduces the time to scrape the player pages from around a fully day down to a few hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_players = scrape.get_all_player_records()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the result is a deeply nested dictionary, we will now convert this to a multi-indexed dataframe. We will also perform a few other miscellaneous cleaning functions, like casting all of the string-format numerical entries to floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_df = scrape.convert_dict_to_df(all_players)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining \"Consistency\" in a Statistical Context"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does \"consistency\" mean for a basketball player? We might say that a perfectly consistent player a) plays every game, and b) performs just about the same way every game. We can measure this in several ways:\n",
    "\n",
    "1. Standard Deviation: Essentially, the standard deviation of the statistic across all games. We can normalize this value to a 0-1 scale by dividing by half the maximum value of the distribution (i.e., the maximum value for the standard deviation).\n",
    "\n",
    "2. Shannon Entropy: Essentially, a measure of how evenly the probability of a given event is distributed across possible outcomes, as given by the formula:\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "We can extend this idea to a basketball season by dividing the total of the statistic for a given game by the total for the whole season, and then taking the log in the base of the number of games that season.\n",
    "\n",
    "3. Unique Values: We can divide the number of unique values for a statistic, then divide by the number of games.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These methods have relative advantages and disadvantages. While all will rate a player who performs exactly the same way in every game as the most consistent, they will have different views of who is the most inconsistent player. Entropy will maximize inconsistency for a player who concentrates all performance into a single game. Standard deviation will maximize inconsistency for a player who vacillates between his best and worst performances equally. Uniqueness will maximize inconsistency for a player who never has the same performance twice.\n",
    "\n",
    "Let's take the following players as examples, looking solely at points:\n",
    "\n",
    "1. Player A: 82 games, 10 pts every game\n",
    "2. Player B: 82 games, 30 pts in 1 game and 0 in the other 81\n",
    "3. Player C: 82 games, with random point totals between 0 and 30\n",
    "4. Player D: 30 DNPs, 25 games with 0 pts, and 27 games with random point totals between 0 and 30\n",
    "5. Player E: 82 games, 41 games with 30 pts, 41 games with 0 pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dict = {\n",
    "    'Player A': np.repeat(10, 82),\n",
    "    'Player B': np.append(np.zeros(82), [30]),\n",
    "    'Player C': np.array(random.choices(range(30), k=82)),\n",
    "    'Player D': np.append(np.append(np.empty(30), np.zeros(25)), np.array(random.choices(range(30), k=27))),\n",
    "    'Player E': np.append(np.repeat(30, 41), np.zeros(41))\n",
    "}\n",
    "score_dict = {key: {'entropy_score': calculate.measure_vector_entropy(value, base=82),\n",
    "                    'deviation_score': calculate.measure_vector_deviation(value),\n",
    "                    'unique_score': calculate.measure_vector_uniqueness(value)\n",
    "                   } for key, value in list(sample_dict.items())}\n",
    "score_df = pd.DataFrame(score_dict).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "width = 0.25\n",
    "x = np.arange(len(score_df))\n",
    "p1 = ax.bar(x - width, score_df['entropy_score'], width, label='Entropy_Score')\n",
    "p2 = ax.bar(x, score_df['deviation_score'], width, label='Deviation_Score')\n",
    "p3 = ax.bar(x + width, score_df['unique_score'], width, label='Uniqueness_Score')\n",
    "ax.set_ylabel('Scores')\n",
    "ax.set_title('Consistency Scores for Sample Players')\n",
    "ax.set_xticklabels([''] + score_df.index.tolist())\n",
    "ax.legend()\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The absolute values here matter less than the relative ones. As we can see, ranking from most to least consistent, Deviation ranks A, B, C, D, E; Entropy ranks A, C, E, D, B; and Uniqueness ranks A, B, E, D, C. Which of these feels like the \"best\" ranking? It's hard to say. The nebulousness of the concept of \"consistency\" may be one reason why fans and analysts use the term so haphazardly.\n",
    "\n",
    "For the purpose of this exercise, I will use a combination of variance and entropy to define consistency. I will not use uniqueness, simply because for lower-total statistics where the vast majority of totals will be concentrated in single-digit values (e.g., assists), this statistic will lose a great deal of its descriptive power. Entropy and standard deviation have enough drawbacks used on their own that it might not be wise to use either without the other. Perhaps the most obvious case is for a player who only plays 1 game. Entropy has no choice but to declare this player to have a low-consistency score (all data concentrated in one spot), while standard deviation will give it a high-consistency score whether we count all missed games as 0 (in which case most games are close to the near-0 average) or ignore them (in which case the single game is consistent with itself)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting and Normalizing Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "basketball-reference.com gives us just about every officially-recorded statistic at the gamelog level. However, we won't necessary want to use all of these statistics in our analysis (or at least not in their rawest forms), for the following reasons:\n",
    "\n",
    "1. Leakage: Some statistics are directly correlated, like field-goals made and total points. Others are more indirectly related but still obviously correlated, like minutes played with every other statistic.\n",
    "\n",
    "2. Clarity: We want to emphasize the major counting statistics rather than incidental counting statistics (e.g., personal fouls) or non-counting statistics (e.g., plus/minus)\n",
    "\n",
    "3. Robustness: Not all statistics have been kept consistently over time. For example, the recording of turnovers, blocks, and steals, as well as the distinction between offensive and defensive rebounds, didn't occur until the mid-1970s. This will make it difficult to use any of them in our analysis. In particular, versions of this model that attempted to incorporate steals and blocks always ended up privileging either the pre-70s or post-70s players, because steals and blocks (likely due to their relatively low raw totals in a given game) are distributed so differently from other statistics that they could not help but bias the results.\n",
    "\n",
    "4. Spread: We will also need to make sure that the scores are sensibly and comparably distributed. For example, in practice, the absolute entropy score for any player who plays regularly will likely tend to be high, because it is simply not plausible for such a player to score all of their points in a small number of games. Similarly, entropy might not be distributed the same way for, say, points as for assists. If we want scores to have similar weight in differentiating players, we will need them all to have a substantial (and preferably similar) distribution of values. This means we may have to transform the raw values.\n",
    "\n",
    "5. Historical Accidents: Entropy requires a base, and to normalize scores to a 0-1 range we should use the number of possible games as the base. However, not every season has the same number of games. So we will have to dynamically change the base to align with the total number of games played by each team each season."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of the exercise, we will analyze only the Big Three statistics of total points, rebounds, and assists.\n",
    "\n",
    "Let's see how deviation and entropy scores are distributed on a player-season level for those three statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "player_df['total_games'] = player_df['year'].apply(lambda x: calculate.get_games_from_year(x))\n",
    "test_set = player_df.loc[player_df.total_games == 82,:]\n",
    "ent_distribution = test_set.groupby(['name', 'player_key', 'year'])[[*calculate.norm_factor]] \\\n",
    "    .agg(lambda vec: calculate.measure_vector_entropy(vec, base=82))\n",
    "std_distribution = test_set.groupby(['name', 'player_key', 'year'])[[*calculate.norm_factor]] \\\n",
    "    .agg(lambda vec: calculate.measure_vector_deviation(vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in std_distribution.columns:\n",
    "    plt.hist(std_distribution[col], bins=20)\n",
    "    plt.title('{feat} Deviation Distribution'.format(feat=col))\n",
    "    plt.xlabel('Deviation Score')\n",
    "    plt.ylabel('Number of Player-Seasons')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the deviation scores are, by eye, close enough to normal that we can be content with them as is. We needn't worry too much about transforming the data. Now let's look at entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ent_distribution.columns:\n",
    "    plt.hist(ent_distribution[col], bins=20)\n",
    "    plt.title('{feat} Entropy Distribution'.format(feat=col))\n",
    "    plt.xlabel('Entropy Score')\n",
    "    plt.ylabel('Number of Player-Seasons')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same is not true of the entropy scores, which is likely due (as mentioned above) to the inherent bias toward high scores. These distributions will not easily normalize. Box-Cox, for example, tends not to improve the situation. As a kind of fudge-factor compromise, the best we can do here is to apply an exponent onto the raw total to force the scores toward essentially the same (non-normal) distribution, where the highest density in the distribution is around 0.8 and the median is around 0.67. This occurs if we apply a power transformation of 3.5 to points, 3 to rebounds, and 2 to assists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ent_distribution.columns:\n",
    "    print(\"Mod median of {col} is {med}\".format(col=col, med=np.nanmedian([ent_distribution[col] ** calculate.norm_factor[col]])))\n",
    "    plt.hist(ent_distribution[col] ** calculate.norm_factor[col], bins=20)\n",
    "    plt.title('Modified {feat} Entropy Distribution'.format(feat=col))\n",
    "    plt.xlabel('Entropy Score')\n",
    "    plt.ylabel('Number of Player-Seasons')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because entropy scores will still end up higher and more clustered, still, than deviation scores, we will square the final entropy score. This will make the median score somewhat lower than the deviation score, but although the most common scores will cluster a bit higher. This will have the effect of roughly equalizing the impact on the final scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run the model and start asking it questions. For example, who had the most consistent season in NBA history?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_consistency = player_df.groupby(['name', 'player_key', 'year']).agg(calculate.measure_group_consistency, group='player_season').reset_index()\n",
    "season_consistency['player_year'] = season_consistency['name'] + '_' + season_consistency['year'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_15 = season_consistency.sort_values('consistency_score', ascending=False).iloc[:15,:] \\\n",
    "    .sort_values('consistency_score')\n",
    "\n",
    "plt.barh(top_15['player_year'], top_15['consistency_score'], color='orange')\n",
    "plt.title('Most Consistent Player-Seasons')\n",
    "plt.ylabel('Player/Year')\n",
    "plt.xlabel('Consistency Score')\n",
    "plt.xlim(0.75, 0.85)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results make intuitive sense, and the fact that they span the game's history indicates that there doesn't seem to be too much era-based bias in the method. All of the players have reputations as reliable, star-level performers, and the fact that several appear multiple times (Malone, Robertson, and Abdul-Jabbar) shows a degree of meta-consistency. But there are some surprises here. 2002, for example, was a middling year for Gary Payton's Seattle Supersonics, bringing the point guard his usual individual accolades but little attention beyond that. However, he was remarkably reliable that year. He played and started all 82 games; he averaged 22 ppg, never scoring fewer than 11 and only scoring more than 30 5 times; he averaged 9 assists, with only two games below 5 and two above 13; and he averaged just under 5 rebounds, with only 5 games above 7. Who at the time registered that he was having perhaps the most consistent year in NBA history?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's ask another question: who has had the most consistency on a season-to-season basis? In other words, whether or not they were consistent in the individual games of a season, who maintained consistent final season totals over the course of their career? Because we will want to insist that players had lengthy careers as a criterion for consistency, we will use 10 as the entropy base for players who played ten or fewer seasons, or use the player's total number of seasons as the base for those with longer careers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_season_consistency = player_df.groupby(['name', 'player_key']).apply(calculate.measure_group_consistency, group='sum_season').reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_15 = full_season_consistency.sort_values('consistency_score', ascending=False).iloc[:15,:] \\\n",
    "    .sort_values('consistency_score')\n",
    "\n",
    "plt.barh(top_15['name'], top_15['consistency_score'], color='orange')\n",
    "plt.title('Most Consistent Player Season-Totals')\n",
    "plt.ylabel('Player/Year')\n",
    "plt.xlabel('Consistency Score')\n",
    "plt.xlim(0.75, 0.87)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That Lebron James is the most consistent player in history by this metric is unsurprising, as he has played consistently at a star level for 18 years, but many other players on this list were role players, and in some cases were even considered as disappointments. Take Marvin Williams, famously taken second in the 2005 draft (ahead of Chris Paul) and widely regarded as a bust. Yet he played for 15 years in the league, always recording at least 50 games per year and consistently notching about 800 points, 375 rebounds, and 90 assists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last, let's look at game-level consistency across an entire career. Who reliably put up their numbers, game-in and game-out, over the course of a decade or more? For this exercise, we will use as our entropy base the total number of games possible in each season the player was active, with a minimum of 820 (ten normal 82-game seasons)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "career_consistency = player_df.groupby(['name', 'player_key']).agg(calculate.measure_group_consistency, group='career').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_15 = career_consistency.sort_values('consistency_score', ascending=False).iloc[:15,:] \\\n",
    "    .sort_values('consistency_score')\n",
    "\n",
    "plt.barh(top_15['name'], top_15['consistency_score'], color='orange')\n",
    "plt.title('Most Consistent Careers')\n",
    "plt.ylabel('Player/Year')\n",
    "plt.xlabel('Consistency Score')\n",
    "plt.xlim(0.72, 0.81)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This list also has a few surprising results. The presence of James, Malone, Duncan, and Abdul-Jabbar in the Top 5 is as expected, as these have been the NBA's gold-standard iron men superstars. But Wilt Chamberlain is a surprise at #3, given the three very different phases of his career (the Warriors' record-breaking scorer, the 76ers' triple-double machine, and the Lakers' defensive anchor). His remarkably consistent rebounding score helps him stay high on the leaderboard. Furthermore, 1980s journeyman guard Reggie Theus stands out as an unexpected entry at #6. Though he labored largely unwatched for the pre-Jordan Bulls and in-transition Sacremento Kings, he maintained remarkably consistent averages of around 18 ppg, 6 apg, and 3 rpg for over a decade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this exercise tell us? Perhaps not much in the way of new insights about the game. But I prefer quantitative precision in language and always want to test vague impressions against data. In many cases this metric confirms what we already thought. But in others, it brings to light unexpected or forgotten achievements. (Alvan Adams, your work has been remembered!) And at the least it can settle bar bets: Hakeem Olajuwon may have been a better player than Patrick Ewing, but at least Ewing was very slightly more consistent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "career_consistency.loc[career_consistency.player_key == 'ewingpa01',:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "career_consistency.loc[career_consistency.name == 'Hakeem Olajuwon',:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
